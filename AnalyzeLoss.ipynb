{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.measure import compare_psnr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp_dir = './output_images/sharp/'\n",
    "iter1_dir = './output_images/iter_1/'\n",
    "iter9_dir = './output_images/iter_9/'\n",
    "blur_dir = './output_images/blur/'\n",
    "\n",
    "inp_ims = os.listdir(inp_dir)\n",
    "iter1_ims = os.listdir(iter1_dir)\n",
    "iter9_ims = os.listdir(iter9_dir)\n",
    "blur_ims = os.listdir(blur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "f_inp, f_iter1, f_iter9, f_blur = list(), list(), list(), list()\n",
    "for f in inp_ims:\n",
    "    base_name = '_'.join(f.split('_')[1:])\n",
    "    i1_found, i9_found, blur_found = False, False, False\n",
    "    for fi1 in iter1_ims:\n",
    "        if base_name in fi1:\n",
    "            i1_found = True\n",
    "            break\n",
    "    for fi9 in iter9_ims:\n",
    "        if base_name in fi9:\n",
    "            i9_found = True\n",
    "            break\n",
    "    for fb in blur_ims:\n",
    "        if base_name in fb:\n",
    "            blur_found = True\n",
    "            break\n",
    "    if i1_found and i9_found:\n",
    "        f_inp.append(f)\n",
    "        f_iter1.append(fi1)\n",
    "        f_iter9.append(fi9)\n",
    "        f_blur.append(fb)\n",
    "        \n",
    "print(len(f_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inp_transform(fname, show=False, rotate=False):\n",
    "    image = Image.open(fname)  #because one sharp image for multiple training images\n",
    "    if show:\n",
    "        image.show()\n",
    "    if rotate:\n",
    "        image = image.rotate(270)\n",
    "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if show:\n",
    "            image.show()\n",
    "    image = np.array(image)\n",
    "    print(image.shape)\n",
    "    image = image[:, :, :3] # 4th channel is transparency... cut it out\n",
    "    image = transforms.ToTensor()(image)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_test1(sharp, blur):\n",
    "    num_examples, channels, width, height = 1, 3, 256, 256\n",
    "    assert(sharp.size() == blur.size())\n",
    "    norm_factor = 1./(num_examples*channels*width*height)\n",
    "    s = blur-sharp\n",
    "    n = torch.norm(s)\n",
    "    print(n)\n",
    "    m = torch.sum(n)\n",
    "    \n",
    "    return norm_factor*m\n",
    "\n",
    "\n",
    "def loss_test2(sharp, blur):\n",
    "    num_examples, channels, width, height = 10, 3, 256, 256\n",
    "    assert(sharp.size() == blur.size())\n",
    "    norm_factor = 1./(num_examples*channels*width*height)\n",
    "    s = (blur-sharp)**2\n",
    "#     n = torch.norm(s)\n",
    "#     print(n)\n",
    "    m = torch.sum(s)\n",
    "    \n",
    "    return norm_factor*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = inp_transform(inp_dir + f_inp[index], show=True)\n",
    "a = inp_transform(iter1_dir + f_iter1[index], show=True, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharp_c_n07881800_12870.JPEG\n",
      "out_ws4_c_n07881800_12870.JPEG\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "(256, 256, 4)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 3840, 3)\n",
      "torch.Size([10, 3, 256, 3840])\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(f_inp[index])\n",
    "print(f_iter1[index])\n",
    "\n",
    "sharp_ims, b1_ims, b9_ims, blur_ims = list(), list(), list(), list()\n",
    "for index in range(len(f_inp)):\n",
    "    sharp_ims.append(inp_transform(inp_dir + f_inp[index]))\n",
    "    b1_ims.append(inp_transform(iter1_dir + f_iter1[index], rotate=True))\n",
    "    b9_ims.append(inp_transform(iter9_dir + f_iter9[index], rotate=True))\n",
    "    blur_ims.append(inp_transform(blur_dir + f_blur[index]))\n",
    "sharp_ims = torch.stack(sharp_ims)\n",
    "b1_ims = torch.stack(b1_ims)\n",
    "b9_ims = torch.stack(b9_ims)\n",
    "blur_ims = torch.stack(blur_ims)\n",
    "\n",
    "print(blur_ims.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056699238576142244"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test2(sharp_ims, b1_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0531722666494911"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test2(sharp_ims, b9_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047725080180383725"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = 4\n",
    "loss_test2(sharp_ims, blur_ims[:, :, :, select*256:(select+1)*256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_image = Image.open(blur_dir + f_blur[index])\n",
    "input_image = np.array(input_image)\n",
    "\n",
    "input_image = np.reshape(input_image, (256, 256, 15, 3), order='F')\n",
    "show_from_numpy(input_image[:, :, 6])\n",
    "input_image = input_image[:, :, 3:13, :]\n",
    "input_image = np.swapaxes(input_image, 2, 3)\n",
    "input_image = np.reshape(input_image, (256, 256, 30), order='F')\n",
    "# input_image.show(input_image[:, :, :3])\n",
    "input_image = transforms.ToTensor()(input_image)\n",
    "demo_image = transforms.ToPILImage()(input_image)\n",
    "demo_image.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_from_numpy(demo_image):\n",
    "    demo_image = transforms.ToTensor()(demo_image)\n",
    "    demo_image = transforms.ToPILImage()(demo_image)\n",
    "    demo_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = Image.open(blur_dir + f_blur[index])\n",
    "input_image = np.array(input_image)\n",
    "input_image = np.reshape(input_image, (256, 256, 15, 3), order='F')\n",
    "show_from_numpy(input_image[:, :, 7])\n",
    "input_image = np.reshape(input_image, (256, 256, 45), order='C')\n",
    "show_from_numpy(input_image[:, :, 12:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_psnr(sharp, blur):\n",
    "    size = 10\n",
    "    for i in range(size):\n",
    "        sharp_im = transforms.To\n",
    "    return torch.sum([compare_psnr(transforms.ToPILImage()(sharp[i]), transforms.ToPILImage()(blur[i])) for i in range(size)])\n",
    "#     return compare_psnr(sharp, blur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-606a7d35d05e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcompute_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharp_ims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1_ims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-124-6bf9157ecd5b>\u001b[0m in \u001b[0;36mcompute_psnr\u001b[0;34m(sharp, blur)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompare_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     return compare_psnr(sharp, blur)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-124-6bf9157ecd5b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblur\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcompare_psnr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msharp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPILImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblur\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#     return compare_psnr(sharp, blur)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/skimage/measure/simple_metrics.py\u001b[0m in \u001b[0;36mcompare_psnr\u001b[0;34m(im_true, im_test, data_range, dynamic_range)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0m_assert_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdynamic_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         warn('`dynamic_range` has been deprecated in favor of '\n",
      "\u001b[0;32m~/anaconda/envs/py36/lib/python3.6/site-packages/skimage/measure/simple_metrics.py\u001b[0m in \u001b[0;36m_assert_compatible\u001b[0;34m(im1, im2)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_compatible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"\"\"Raise an error if the shape and dtype do not match.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input images must have the same dtype.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mim1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mim2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "compute_psnr(sharp_ims, b1_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "s = sharp_ims.numpy()\n",
    "b = blur_ims.numpy()\n",
    "\n",
    "print(s.shape)\n",
    "s0 = s[0]\n",
    "# compute_psnr(s[0], b[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
