{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inp_dir = './output_images/sharp/'\n",
    "iter1_dir = './output_images/iter_1/'\n",
    "iter9_dir = './output_images/iter_9/'\n",
    "blur_dir = './output_images/blur/'\n",
    "\n",
    "inp_ims = os.listdir(inp_dir)\n",
    "iter1_ims = os.listdir(iter1_dir)\n",
    "iter9_ims = os.listdir(iter9_dir)\n",
    "blur_ims = os.listdir(blur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "f_inp, f_iter1, f_iter9, f_blur = list(), list(), list(), list()\n",
    "for f in inp_ims:\n",
    "    base_name = '_'.join(f.split('_')[1:])\n",
    "    i1_found, i9_found, blur_found = False, False, False\n",
    "    for fi1 in iter1_ims:\n",
    "        if base_name in fi1:\n",
    "            i1_found = True\n",
    "            break\n",
    "    for fi9 in iter9_ims:\n",
    "        if base_name in fi9:\n",
    "            i9_found = True\n",
    "            break\n",
    "    for fb in blur_ims:\n",
    "        if base_name in fb:\n",
    "            blur_found = True\n",
    "            break\n",
    "    if i1_found and i9_found:\n",
    "        f_inp.append(f)\n",
    "        f_iter1.append(fi1)\n",
    "        f_iter9.append(fi9)\n",
    "        f_blur.append(fb)\n",
    "        \n",
    "print(len(f_inp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inp_transform(fname, show=False, rotate=False):\n",
    "    image = Image.open(fname)  #because one sharp image for multiple training images\n",
    "    if show:\n",
    "        image.show()\n",
    "    if rotate:\n",
    "        image = image.rotate(270)\n",
    "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if show:\n",
    "            image.show()\n",
    "    image = np.array(image)\n",
    "    image = image[:, :, :3] # 4th channel is transparency... cut it out\n",
    "    image = transforms.ToTensor()(image)\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss_test1(sharp, blur):\n",
    "    num_examples, channels, width, height = 1, 3, 256, 256\n",
    "    assert(sharp.size() == blur.size())\n",
    "    norm_factor = 1./(num_examples*channels*width*height)\n",
    "    s = blur-sharp\n",
    "    n = torch.norm(s)\n",
    "    print(n)\n",
    "    m = torch.sum(n)\n",
    "    \n",
    "    return norm_factor*m\n",
    "\n",
    "\n",
    "def loss_test2(sharp, blur):\n",
    "    num_examples, channels, width, height = 10, 3, 256, 256\n",
    "    assert(sharp.size() == blur.size())\n",
    "    norm_factor = 1./(num_examples*channels*width*height)\n",
    "    s = (blur-sharp)**2\n",
    "#     n = torch.norm(s)\n",
    "#     print(n)\n",
    "    m = torch.sum(s)\n",
    "    \n",
    "    return norm_factor*m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = inp_transform(inp_dir + f_inp[index], show=True)\n",
    "a = inp_transform(iter1_dir + f_iter1[index], show=True, rotate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sharp_c_n07881800_12870.JPEG\n",
      "out_ws4_c_n07881800_12870.JPEG\n",
      "torch.Size([10, 3, 256, 3840])\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(f_inp[index])\n",
    "print(f_iter1[index])\n",
    "\n",
    "sharp_ims, b1_ims, b9_ims, blur_ims = list(), list(), list(), list()\n",
    "for index in range(len(f_inp)):\n",
    "    sharp_ims.append(inp_transform(inp_dir + f_inp[index]))\n",
    "    b1_ims.append(inp_transform(iter1_dir + f_iter1[index], rotate=True))\n",
    "    b9_ims.append(inp_transform(iter9_dir + f_iter9[index], rotate=True))\n",
    "    blur_ims.append(inp_transform(blur_dir + f_blur[index]))\n",
    "sharp_ims = torch.stack(sharp_ims)\n",
    "b1_ims = torch.stack(b1_ims)\n",
    "b9_ims = torch.stack(b9_ims)\n",
    "blur_ims = torch.stack(blur_ims)\n",
    "\n",
    "print(blur_ims.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharp_im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 256, 256])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_im.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056699238576142244"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test2(sharp_ims, b1_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0531722666494911"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_test2(sharp_ims, b9_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047725080180383725"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "select = 4\n",
    "loss_test2(sharp_ims, blur_ims[:, :, :, select*256:(select+1)*256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_image = Image.open(blur_dir + f_blur[index])\n",
    "input_image = np.array(input_image)\n",
    "\n",
    "input_image = np.reshape(input_image, (256, 256, 15, 3), order='F')\n",
    "show_from_numpy(input_image[:, :, 6])\n",
    "input_image = input_image[:, :, 3:13, :]\n",
    "input_image = np.swapaxes(input_image, 2, 3)\n",
    "input_image = np.reshape(input_image, (256, 256, 30), order='F')\n",
    "# input_image.show(input_image[:, :, :3])\n",
    "input_image = transforms.ToTensor()(input_image)\n",
    "demo_image = transforms.ToPILImage()(input_image)\n",
    "demo_image.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_from_numpy(demo_image):\n",
    "    demo_image = transforms.ToTensor()(demo_image)\n",
    "    demo_image = transforms.ToPILImage()(demo_image)\n",
    "    demo_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_image = Image.open(blur_dir + f_blur[index])\n",
    "input_image = np.array(input_image)\n",
    "input_image = np.reshape(input_image, (256, 256, 15, 3), order='F')\n",
    "show_from_numpy(input_image[:, :, 7])\n",
    "input_image = np.reshape(input_image, (256, 256, 45), order='C')\n",
    "show_from_numpy(input_image[:, :, 12:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
